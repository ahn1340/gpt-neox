{
   "pipe_parallel_size": 1,
   "model_parallel_size": 4,

   "num_layers": 32,
   "hidden_size": 4096,
   "num_attention_heads": 32,
   "seq_length": 2048,
   "max_position_embeddings": 2048,
   "norm": "layernorm",
   "pos_emb": "rotary",
   "rotary_pct": 0.25,
   "no_weight_tying": true,
   "gpt_j_residual": true,
   "output_layer_parallelism": "column",

   "attention_config": [[["flash"], 32]],

   "scaled_upper_triang_masked_softmax_fusion": true,
   "bias_gelu_fusion": true,

   "optimizer": {
     "type": "Adam",
     "params": {
       "lr": 0.00012,
       "betas": [0.9, 0.95],
       "eps": 1.0e-8
     }
   },
   "min_lr": 0.000012,

   "zero_optimization": {
    "stage": 1,
    "allgather_partitions": true,
    "allgather_bucket_size": 1260000000,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 1260000000,
    "contiguous_gradients": true,
    "cpu_offload": false
  },

   "train_batch_size": 1024,
   "train_micro_batch_size_per_gpu": 4,
   "gradient_accumulation_steps": 8,

   "checkpoint_activations": false,
   "checkpoint_num_layers": 1,
   "partition_activations": true,
   "synchronize_each_layer": true,

   "gradient_clipping": 1.0,
   "weight_decay": 0.1,
   "hidden_dropout": 0,
   "attention_dropout": 0,

   # bf16
   "precision": "bfloat16",
   "fp32_allreduce": True, # without a patch to torch, bf16 models have to do the allreduce in fp32
   "data_types": {"grad_accum_dtype": "fp32"},

   #"fp16": {
   #  "fp16": true,
   #  "enabled": true,
   #  "loss_scale": 0,
   #  "loss_scale_window": 1000,
   #  "initial_scale_power": 12,
   #  "hysteresis": 2,
   #  "min_loss_scale": 1
   #},

   "train_iters": 715000,
   "lr_decay_iters": 715000,
   "keep_last_n_checkpoints": 1,
   "distributed_backend": "nccl",
   "lr_decay_style": "cosine",
   "warmup": 0.005,
   "checkpoint_factor": 5000,
   "eval_interval": 715000,
   "eval_iters": 100,

   "log_interval": 1,
   "steps_per_print": 1,
   "wall_clock_breakdown": true,

   "tokenizer_type": "HFTokenizer",

  # train config
  "train_data_paths": ['/weka/home-jinwooahn/idxs/merge-Blog',
  '/weka/home-jinwooahn/idxs/merge-Community',
  '/weka/home-jinwooahn/idxs/merge-Dialogue',
  '/weka/home-jinwooahn/idxs/merge-Encyclopedia',
  '/weka/home-jinwooahn/idxs/merge-Miscellaneous',
  '/weka/home-jinwooahn/idxs/merge-News',
  '/weka/home-jinwooahn/idxs/merge-Prose',
  '/weka/home-jinwooahn/idxs/merge-Web',
  '/weka/home-jinwooahn/idxs/refined-00.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-01.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-02.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-03.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-04.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-05.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-06.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-07.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-08.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-09.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-10.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-11.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-13.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-14.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-15.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-16.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-17.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-18.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-19.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-20.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-21.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-22.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-23.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-24.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-25.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-26.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-27.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-28.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/refined-30.jsonl_text_document',
  '/weka/home-jinwooahn/idxs/slim-pajama_text_document',
  '/weka/home-jinwooahn/idxs/the-stack-dedup-01_text_document'],
  "train_data_weights": [2179.037246,
  133.1279,
  0.6580,
  24.4105,
  93.5836,
  266.5269,
  78.5888,
  294.1938,
  159.7153,
  160.5517,
  160.5362,
  160.6355,
  159.8202,
  160.8010,
  160.5858,
  158.9365,
  158.8516,
  158.7736,
  158.4286,
  159.0938,
  159.2720,
  158.8843,
  158.8274,
  157.8932,
  159.3024,
  159.3323,
  158.0635,
  158.9889,
  159.0800,
  159.9745,
  159.2578,
  161.4557,
  158.7702,
  158.4028, 
  158.7384,
  159.3969,
  0.000008,
  1779.7136,
  687.7878],
  # use small scale (50B) dataset as valid and test set.
  "valid_data_paths": ["/admin/home-jinwooahn/repos/data/indexes/mmap/small_exp_50B"],
  "test_data_paths": ["/admin/home-jinwooahn/repos/data/indexes/mmap/small_exp_50B"],
  # upsample Korean data
  "data_impl": "s3",
  "vocab_file": "/weka/home-jinwooahn/tokenizer/polyglot_bpe_50k.json",
  "save": "/weka/home-jinwooahn/polyglot-v2/checkpoints",  # if this is not set, checkpoint does not save at all
  "s3_path": "s3://polyglot-korean-west/polyglot-v2",
  "checkpoint_validation_with_forward_pass": false,
  "tensorboard_dir": "tensorboard",
  "log_dir": "logs",
  "launcher": "slurm",
  "deepspeed_slurm": true,
  "no_ssh_check": true,
  "use_wandb": true,
  "wandb_host": "https://api.wandb.ai",
  "wandb_project": "polyglot-v2-7B",
  # resume params
  "load": '/weka/home-jinwooahn/polyglot-v2/checkpoints',  # automatically loads latest checkpoint
  "use_checkpoint_lr_scheduler": true,
}