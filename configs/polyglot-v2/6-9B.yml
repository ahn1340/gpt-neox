{
   "pipe_parallel_size": 1,
   "model_parallel_size": 4,

   "num_layers": 32,
   "hidden_size": 4096,
   "num_attention_heads": 32,
   "seq_length": 2048,
   "max_position_embeddings": 2048,
   "norm": "layernorm",
   "pos_emb": "rotary",
   "rotary_pct": 0.25,
   "no_weight_tying": true,
   "gpt_j_residual": true,
   "output_layer_parallelism": "column",

   "attention_config": [[["flash"], 32]],

   "scaled_upper_triang_masked_softmax_fusion": true,
   "bias_gelu_fusion": true,

   "optimizer": {
     "type": "Adam",
     "params": {
       "lr": 0.00012,
       "betas": [0.9, 0.95],
       "eps": 1.0e-8
     }
   },
   "min_lr": 0.000012,

   "zero_optimization": {
    "stage": 1,
    "allgather_partitions": true,
    "allgather_bucket_size": 1260000000,
    "overlap_comm": true,
    "reduce_scatter": true,
    "reduce_bucket_size": 1260000000,
    "contiguous_gradients": true,
    "cpu_offload": false
  },

   "train_batch_size": 1024,
   "train_micro_batch_size_per_gpu": 4,
   "gradient_accumulation_steps": 8,

   "checkpoint_activations": false,
   "checkpoint_num_layers": 1,
   "partition_activations": true,
   "synchronize_each_layer": true,

   "gradient_clipping": 1.0,
   "weight_decay": 0.1,
   "hidden_dropout": 0,
   "attention_dropout": 0,

   # bf16
   "precision": "bfloat16",
   "fp32_allreduce": True, # without a patch to torch, bf16 models have to do the allreduce in fp32
   "data_types": {"grad_accum_dtype": "fp32"},

   #"fp16": {
   #  "fp16": true,
   #  "enabled": true,
   #  "loss_scale": 0,
   #  "loss_scale_window": 1000,
   #  "initial_scale_power": 12,
   #  "hysteresis": 2,
   #  "min_loss_scale": 1
   #},

   "train_iters": 715000,
   "lr_decay_iters": 715000,
   "keep_last_n_checkpoints": 1,
   "distributed_backend": "nccl",
   "lr_decay_style": "cosine",
   "warmup": 0.01,
   "checkpoint_factor": 5000,
   "eval_interval": 5000,
   "eval_iters": 100,

   "log_interval": 1,
   "steps_per_print": 1,
   "wall_clock_breakdown": true,

   "tokenizer_type": "HFTokenizer",
}
